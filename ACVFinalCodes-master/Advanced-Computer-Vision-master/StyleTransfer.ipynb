{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"StyleTransfer.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YmAYFSDwqu7K","colab_type":"text"},"source":["#### IMPORT THE LIBRARIES"]},{"cell_type":"code","metadata":{"id":"VjUvvJoQqu7N","colab_type":"code","colab":{}},"source":["from torchvision import models\n","from torchvision import transforms\n","from PIL import Image\n","import argparse\n","import torch\n","from torch import optim\n","import torchvision\n","import torch.nn as nn\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZE2jeCsFqu7S","colab_type":"text"},"source":["#### DEFINING THE VGG16 MODEL"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HEqsV8KMK29H","colab":{}},"source":["\n","# define the VGG\n","class VGG(nn.Module):\n","    def __init__(self):\n","        super(VGG, self).__init__()\n","        \n","        # load the vgg model's features\n","        self.vgg = models.vgg19(pretrained=True).features\n","    \n","    def get_content_activations(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","            Extracts the features for the content loss from the block4_conv2 of VGG19\n","            Args:\n","                x: torch.Tensor - input image we want to extract the features of\n","            Returns:\n","                features: torch.Tensor - the activation maps of the block4_conv2 layer\n","        \"\"\"\n","        features = self.vgg[:23](x)\n","        return features\n","    \n","    def get_style_activations(self, x):\n","        \"\"\"\n","            Extracts the features for the style loss from the block1_conv1, \n","                block2_conv1, block3_conv1, block4_conv1, block5_conv1 of VGG19\n","            Args:\n","                x: torch.Tensor - input image we want to extract the features of\n","            Returns:\n","                features: list - the list of activation maps of the block1_conv1, \n","                    block2_conv1, block3_conv1, block4_conv1, block5_conv1 layers\n","        \"\"\"\n","        features = [self.vgg[:4](x)] + [self.vgg[:7](x)] + [self.vgg[:12](x)] + [self.vgg[:21](x)] + [self.vgg[:30](x)] \n","        return features\n","    \n","    def forward(self, x):\n","        return self.vgg(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oif2w-GrvQOF","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"0dDNtZYoqu7Y","colab_type":"text"},"source":["#### DEFINING THE LOSS FUNCTIONS"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GH6fakD0LBOj","colab":{}},"source":["def gram(tensor):\n","    \"\"\"\n","        Constructs the Gramian matrix out of the tensor\n","    \"\"\"\n","    return torch.mm(tensor, tensor.t())\n","\n","\n","def gram_loss(noise_img_gram, style_img_gram, N, M):\n","    \"\"\"\n","        Gramian loss: the SSE between Gramian matrices of a layer\n","            arXiv:1508.06576v2 - equation (4)\n","    \"\"\"\n","    return torch.sum(torch.pow(noise_img_gram - style_img_gram, 2)).div((np.power(N*M*2, 2, dtype=np.float64)))\n","\n","\n","def total_variation_loss(image):\n","    \"\"\"\n","        Variation loss makes the images smoother, defined over spacial dimensions\n","    \"\"\"\n","    loss = torch.mean(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:])) + \\\n","        torch.mean(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :]))\n","    return loss\n","\n","\n","def content_loss(noise: torch.Tensor, image: torch.Tensor):\n","    \"\"\"\n","        Simple SSE loss over the generated image and the content image\n","            arXiv:1508.06576v2 - equation (1)\n","    \"\"\"\n","    return 1/2. * torch.sum(torch.pow(noise - image, 2))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xevv1Tp2Ln1U","colab":{}},"source":["def main(style_img_path: str,\n","         content_img_path: str, \n","         img_dim: int,\n","         num_iter: int,\n","         style_weight: int,\n","         content_weight: int,\n","         variation_weight: int,\n","         print_every: int,\n","         save_every: int):\n","\n","    assert style_img_path is not None\n","    assert content_img_path is not None\n","\n","    # define the device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    \n","    # read the images\n","    style_img = Image.open(style_img_path)\n","    cont_img = Image.open(content_img_path)\n","    \n","    # define the transform\n","    transform = transforms.Compose([transforms.Resize((img_dim, img_dim)),\n","                                    transforms.ToTensor(), \n","                                    transforms.Normalize([0.485, 0.456, 0.406],\n","                                                         [0.229, 0.224, 0.225])])\n","    \n","    # get the tensor of the image\n","    content_image = transform(cont_img).unsqueeze(0).to(device)\n","    style_image = transform(style_img).unsqueeze(0).to(device)\n","    \n","    # init the network\n","    vgg = VGG().to(device).eval()\n","    \n","    # replace the MaxPool with the AvgPool layers\n","    for name, child in vgg.vgg.named_children():\n","        if isinstance(child, nn.MaxPool2d):\n","            vgg.vgg[int(name)] = nn.AvgPool2d(kernel_size=2, stride=2)\n","            \n","    # lock the gradients\n","    for param in vgg.parameters():\n","        param.requires_grad = False\n","    \n","    # get the content activations of the content image and detach them from the graph\n","    content_activations = vgg.get_content_activations(content_image).detach()\n","    \n","    # unroll the content activations\n","    content_activations = content_activations.view(512, -1)\n","    \n","    # get the style activations of the style image\n","    style_activations = vgg.get_style_activations(style_image)\n","    \n","    # for every layer in the style activations\n","    for i in range(len(style_activations)):\n","\n","        # unroll the activations and detach them from the graph\n","        style_activations[i] = style_activations[i].squeeze().view(style_activations[i].shape[1], -1).detach()\n","\n","    # calculate the gram matrices of the style image\n","    style_grams = [gram(style_activations[i]) for i in range(len(style_activations))]\n","    \n","    # generate the Gaussian noise\n","    noise = torch.randn(1, 3, img_dim, img_dim, device=device, requires_grad=True)\n","    \n","    # define the adam optimizer\n","    # pass the feature map pixels to the optimizer as parameters\n","    adam = optim.Adam(params=[noise], lr=0.01, betas=(0.9, 0.999))\n","\n","    # run the iteration\n","    for iteration in range(num_iter):\n","\n","        # zero the gradient\n","        adam.zero_grad()\n","\n","        # get the content activations of the Gaussian noise\n","        noise_content_activations = vgg.get_content_activations(noise)\n","\n","        # unroll the feature maps of the noise\n","        noise_content_activations = noise_content_activations.view(512, -1)\n","\n","        # calculate the content loss\n","        content_loss_ = content_loss(noise_content_activations, content_activations)\n","\n","        # get the style activations of the noise image\n","        noise_style_activations = vgg.get_style_activations(noise)\n","\n","        # for every layer\n","        for i in range(len(noise_style_activations)):\n","\n","            # unroll the the noise style activations\n","            noise_style_activations[i] = noise_style_activations[i].squeeze().view(noise_style_activations[i].shape[1], -1)\n","\n","        # calculate the noise gram matrices\n","        noise_grams = [gram(noise_style_activations[i]) for i in range(len(noise_style_activations))]\n","\n","        # calculate the total weighted style loss\n","        style_loss = 0\n","        for i in range(len(style_activations)):\n","            N, M = noise_style_activations[i].shape[0], noise_style_activations[i].shape[1]\n","            style_loss += (gram_loss(noise_grams[i], style_grams[i], N, M) / 5.)\n","\n","        # put the style loss on device\n","        style_loss = style_loss.to(device)\n","            \n","        # calculate the total variation loss\n","        variation_loss = total_variation_loss(noise).to(device)\n","\n","        # weight the final losses and add them together\n","        total_loss = content_weight * content_loss_ + style_weight * style_loss + variation_weight * variation_loss\n","\n","        if iteration % print_every == 0:\n","            print(\"Iteration: {}, Content Loss: {:.3f}, Style Loss: {:.3f}, Var Loss: {:.3f}\".format(iteration, \n","                                                                                                     content_weight * content_loss_.item(),\n","                                                                                                     style_weight * style_loss.item(), \n","                                                                                                     variation_weight * variation_loss.item()))\n","\n","        # create the folder for the generated images\n","        if not os.path.exists('./generated/'):\n","            os.mkdir('./generated/')\n","        \n","        # generate the image\n","        if iteration % save_every == 0:\n","            save_image(noise.cpu().detach(), filename='./generated/iter_{}.png'.format(iteration))\n","\n","        # backprop\n","        total_loss.backward()\n","        \n","        # update parameters\n","        adam.step()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XaRtt96IMkPk","outputId":"73d05aec-bc28-40c0-af0a-aef8c34cdf1b","executionInfo":{"status":"ok","timestamp":1573648528850,"user_tz":-330,"elapsed":52723,"user":{"displayName":"AVARI FARZAD","photoUrl":"","userId":"06062050648998271274"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":110}},"source":["from google.colab import files\n","\n","uploaded = files.upload()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-9014ba3f-1749-42b3-bcc2-5ba196d39357\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-9014ba3f-1749-42b3-bcc2-5ba196d39357\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving content4.1.jpg to content4.1.jpg\n","Saving style4.jpg to style4.jpg\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"S9uOVjojNlwI","outputId":"7a967134-0ae3-4019-ea53-47b557537cbd","executionInfo":{"status":"ok","timestamp":1573648564026,"user_tz":-330,"elapsed":3350,"user":{"displayName":"AVARI FARZAD","photoUrl":"","userId":"06062050648998271274"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["content4.1.jpg\tsample_data  style4.jpg\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_fdz68oaqu7u","colab_type":"text"},"source":["#### TRAINING THE MODEL"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5PflNHxPPAW3","outputId":"4bdef75d-9df3-4a45-dcde-b9da1234691a","executionInfo":{"status":"ok","timestamp":1573650250865,"user_tz":-330,"elapsed":1647404,"user":{"displayName":"AVARI FARZAD","photoUrl":"","userId":"06062050648998271274"}},"colab":{"base_uri":"https://localhost:8080/","height":488}},"source":["import os\n","from torchvision.utils import save_image\n","\n","\n","style_img = 'style4.jpg'\n","content_img = 'content4.1.jpg'\n","\n","main(style_img, content_img, 512, 12000, 10e6, 10e-4, 10e3, 500, 1000)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n","100%|██████████| 548M/548M [00:23<00:00, 24.5MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Iteration: 0, Content Loss: 595.661, Style Loss: 6045728.922, Var Loss: 22559.714\n","Iteration: 500, Content Loss: 1558.310, Style Loss: 160533.562, Var Loss: 19695.745\n","Iteration: 1000, Content Loss: 1640.564, Style Loss: 42635.049, Var Loss: 17729.247\n","Iteration: 1500, Content Loss: 1670.279, Style Loss: 21921.599, Var Loss: 15128.016\n","Iteration: 2000, Content Loss: 1685.772, Style Loss: 13277.957, Var Loss: 12235.552\n","Iteration: 2500, Content Loss: 1683.590, Style Loss: 8895.844, Var Loss: 9342.471\n","Iteration: 3000, Content Loss: 1680.252, Style Loss: 6534.718, Var Loss: 6867.905\n","Iteration: 3500, Content Loss: 1677.157, Style Loss: 5009.090, Var Loss: 5235.654\n","Iteration: 4000, Content Loss: 1672.694, Style Loss: 3971.605, Var Loss: 4420.496\n","Iteration: 4500, Content Loss: 1668.296, Style Loss: 3225.525, Var Loss: 4058.207\n","Iteration: 5000, Content Loss: 1663.279, Style Loss: 2677.605, Var Loss: 3883.965\n","Iteration: 5500, Content Loss: 1658.576, Style Loss: 2261.072, Var Loss: 3787.591\n","Iteration: 6000, Content Loss: 1667.765, Style Loss: 3914.873, Var Loss: 4163.406\n","Iteration: 6500, Content Loss: 1649.834, Style Loss: 1706.782, Var Loss: 3664.858\n","Iteration: 7000, Content Loss: 1645.997, Style Loss: 1551.152, Var Loss: 3621.825\n","Iteration: 7500, Content Loss: 1641.740, Style Loss: 1413.511, Var Loss: 3586.769\n","Iteration: 8000, Content Loss: 1637.399, Style Loss: 1288.985, Var Loss: 3555.756\n","Iteration: 8500, Content Loss: 1632.748, Style Loss: 1180.991, Var Loss: 3527.663\n","Iteration: 9000, Content Loss: 1628.834, Style Loss: 1089.260, Var Loss: 3502.427\n","Iteration: 9500, Content Loss: 1626.167, Style Loss: 1032.189, Var Loss: 3512.573\n","Iteration: 10000, Content Loss: 1623.221, Style Loss: 993.599, Var Loss: 3479.539\n","Iteration: 10500, Content Loss: 1621.266, Style Loss: 965.312, Var Loss: 3465.778\n","Iteration: 11000, Content Loss: 1619.586, Style Loss: 936.489, Var Loss: 3454.511\n","Iteration: 11500, Content Loss: 1617.961, Style Loss: 907.598, Var Loss: 3443.837\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DWazIvW21brs","outputId":"bc3e44b9-ef9f-4e50-ca9a-84afad61459b","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["!ls /content/generated/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["iter_0.png     iter_200.png   iter_400.png   iter_600.png\n","iter_1000.png  iter_3000.png  iter_5000.png  iter_7000.png\n","iter_100.png   iter_300.png   iter_500.png   iter_8000.png\n","iter_2000.png  iter_4000.png  iter_6000.png  iter_9000.png\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"x9TkhXjr1nYr","colab":{}},"source":["from google.colab import files\n","files.download('/content/generated/iter_8000.png')"],"execution_count":0,"outputs":[]}]}