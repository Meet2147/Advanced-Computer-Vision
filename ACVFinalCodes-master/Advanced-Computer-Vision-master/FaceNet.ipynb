{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"FaceNet.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fBvDxVNGMVIl","colab_type":"text"},"source":["#### IMPORT THE LIBRARIES"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"x7HjVWQnYsjv","colab":{}},"source":["import numpy as np\n","import cv2\n","import os\n","import model as embedding\n","import torch\n","import pickle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8LEECKCVJtmp","colab":{}},"source":["# upload model.py, deploy.prototxt.txt, res10, dataset.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EsOcygsQ7SfS","outputId":"c2743dea-916e-4ffe-8660-27fb2fb5345b","executionInfo":{"status":"ok","timestamp":1573975021072,"user_tz":-330,"elapsed":27264,"user":{"displayName":"AVARI FARZAD","photoUrl":"","userId":"06062050648998271274"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["!unzip dataset.zip"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Archive:  TrainingSet.zip\n","replace TrainingSet/Devansh_Parikh/14211947_1753240381590885_3547486871695091471_n.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace TrainingSet/Devansh_Parikh/21462751_1939415376306717_5318671981398691252_n.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Oun6DPoY7MLH","colab":{}},"source":["# face detection model paths\n","protoPath = os.getcwd()+\"/deploy.prototxt.txt\"\n","modelPath = os.getcwd()+\"/res10_300x300_ssd_iter_140000.caffemodel\"\n","\n","# loading detection model\n","detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n","\n","# load embedding model\n","embedder = embedding.InceptionResnetV1(pretrained='vggface2').eval()\n","\n","# paths to save pickle files\n","currentDir = os.getcwd()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5Lw2OaiP8Bp7","colab":{}},"source":["# images folder\n","dataset = os.path.join(currentDir, \"dataset\")\n","\n","# paths to save pickle files\n","!mkdir output\n","!touch  output/SimpleEmbeddings.pickle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AZCfFbzb8JC2","colab":{}},"source":["# getting all images paths\n","\n","imagePaths = []\n","\n","for person in os.listdir(dataset):\n","    for img in os.listdir(dataset+\"/\"+person):\n","        imagePaths.append(dataset+\"/\"+person+\"/\"+img)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XtlsxgLk9KAh","colab":{}},"source":["# create lists to append ImgPaths/names/imageIDs/boxs/embeddings\n","ImgPaths = []\n","names = []\n","imageIDs = []\n","boxs = []\n","embeddings = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZOKkmjJ4MVI-","colab_type":"code","outputId":"82a1779b-edec-4bd6-a9dc-30a15523fafd","executionInfo":{"status":"ok","timestamp":1573976323997,"user_tz":-330,"elapsed":1234,"user":{"displayName":"AVARI FARZAD","photoUrl":"","userId":"06062050648998271274"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["imagePaths[98].split(\"/\")[-2]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Rahane'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"8oigLMTnMVJD","colab_type":"code","colab":{}},"source":["names = []\n","for (i, imagePath) in enumerate(imagePaths):\n","    name = imagePath.split(\"/\")[-2]\n","    names.append(name)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IuPyXT_Z9OXC","colab":{}},"source":["# initlize the total number of faces processed\n","total = 0\n","\n","# loop over the image paths\n","for (i, imagePath) in enumerate(imagePaths):\n","  \n","    #print(i,imagePath)\n","    \n","    #extract the person name from the image path\n","    \n","    name = imagePath.split(\"/\")[-2]\n","    imageID = imagePath.split(os.path.sep)[-1].split('.')[-2]\n","    \n","    image = cv2.imread(imagePath)\n","    (h,w) = image.shape[:2]\n","    \n","    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n","    \n","    detector.setInput(blob)\n","    detections = detector.forward()\n","    \n","    if len(detections) > 0:\n","        \n","        # we're making the assumption that each image has only ONE\n","        # face, so find the bounding box with the largest probalility\n","        \n","        i = np.argmax(detections[0, 0, :, 2])\n","        confidence = detections[0, 0, i, 2]\n","        \n","        # ensure that the detection with the largest probability also\n","        # means our minimum probability test (thus helping filter out\n","        # weak detections)\n","        \n","        if confidence > 0.5:\n","            \n","            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","            \n","            (startX, startY, endX, endY) = box.astype(\"int\")\n","            \n","            face = image[startY:endY , startX:endX]\n","            (fH , fW) = face.shape[:2]\n","            \n","            \n","            # ensure the facce width and height are sufficently large\n","            if fW < 20 or fH < 20:\n","                continue\n","                \n","            try:\n","                faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,(160, 160), (0, 0, 0), swapRB=True, crop=False)\n","            except:\n","                print(\"[Error] - Face size in Image not sufficent to get Embeddings : \", imagePath)\n","                continue\n","            \n","            faceTensor = torch.tensor(faceBlob)\n","            faceEmbed = embedder(faceTensor)\n","            flattenEmbed = faceEmbed.squeeze(0).detach().numpy()\n","            \n","            ImgPaths.append(imagePath)\n","            imageIDs.append(imageID)\n","            names.append(name)\n","            boxs.append(box)\n","            embeddings.append(flattenEmbed)\n","            total += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qHdG65SEMVJI","colab_type":"code","outputId":"dfb3f913-bc3a-40d4-a1ae-2ebf5a917d48","executionInfo":{"status":"ok","timestamp":1573976404035,"user_tz":-330,"elapsed":20101,"user":{"displayName":"AVARI FARZAD","photoUrl":"","userId":"06062050648998271274"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["names"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Obulapathi',\n"," 'Obulapathi',\n"," 'prasad',\n"," 'prasad',\n"," 'prasad',\n"," 'prasad',\n"," 'prasad',\n"," 'prasad',\n"," 'prasad',\n"," 'prasad',\n"," 'prasad',\n"," 'prabhat_ranjan',\n"," 'prabhat_ranjan',\n"," 'prabhat_ranjan',\n"," 'prabhat_ranjan',\n"," 'prabhat_ranjan',\n"," 'prabhat_ranjan',\n"," 'prabhat_ranjan',\n"," 'prabhat_ranjan',\n"," 'prabhat_ranjan',\n"," 'modi',\n"," 'modi',\n"," 'modi',\n"," 'modi',\n"," 'modi',\n"," 'modi',\n"," 'modi',\n"," 'modi',\n"," 'modi',\n"," 'modi',\n"," 'B kumar',\n"," 'B kumar',\n"," 'B kumar',\n"," 'B kumar',\n"," 'B kumar',\n"," 'B kumar',\n"," 'B kumar',\n"," 'B kumar',\n"," 'sachin',\n"," 'sachin',\n"," 'sachin',\n"," 'sachin',\n"," 'sachin',\n"," 'sachin',\n"," 'sachin',\n"," 'sachin',\n"," 'sachin',\n"," 'sachin',\n"," 'sachin',\n"," 'DYPatil',\n"," 'DYPatil',\n"," 'DYPatil',\n"," 'DYPatil',\n"," 'DYPatil',\n"," 'DYPatil',\n"," 'DYPatil',\n"," 'DYPatil',\n"," 'DYPatil',\n"," 'DYPatil',\n"," 'raina',\n"," 'raina',\n"," 'raina',\n"," 'raina',\n"," 'raina',\n"," 'raina',\n"," 'raina',\n"," 'raina',\n"," 'viratKohili',\n"," 'viratKohili',\n"," 'viratKohili',\n"," 'viratKohili',\n"," 'viratKohili',\n"," 'viratKohili',\n"," 'viratKohili',\n"," 'viratKohili',\n"," 'viratKohili',\n"," 'viratKohili',\n"," 'bumrah',\n"," 'bumrah',\n"," 'bumrah',\n"," 'bumrah',\n"," 'bumrah',\n"," 'bumrah',\n"," 'bumrah',\n"," 'bumrah',\n"," 'bumrah',\n"," 'Ashwin',\n"," 'Ashwin',\n"," 'Ashwin',\n"," 'Ashwin',\n"," 'Ashwin',\n"," 'Ashwin',\n"," 'Ashwin',\n"," 'Ashwin',\n"," 'Ashwin',\n"," 'Rahane',\n"," 'Rahane',\n"," 'Rahane',\n"," 'Rahane',\n"," 'Rahane',\n"," 'Rahane',\n"," 'Rahane',\n"," 'Rahane',\n"," 'rohit',\n"," 'rohit',\n"," 'rohit',\n"," 'rohit',\n"," 'rohit',\n"," 'rohit',\n"," 'rohit',\n"," 'rohit',\n"," 'rohit',\n"," 'rohit',\n"," 'APJ Abdul Kalam',\n"," 'APJ Abdul Kalam',\n"," 'APJ Abdul Kalam',\n"," 'APJ Abdul Kalam',\n"," 'APJ Abdul Kalam',\n"," 'APJ Abdul Kalam',\n"," 'APJ Abdul Kalam',\n"," 'APJ Abdul Kalam',\n"," 'APJ Abdul Kalam',\n"," 'APJ Abdul Kalam',\n"," 'dhawan',\n"," 'dhawan',\n"," 'dhawan',\n"," 'dhawan',\n"," 'dhawan',\n"," 'dhawan',\n"," 'dhawan',\n"," 'dhawan',\n"," 'dhawan',\n"," 'Umesh',\n"," 'Umesh',\n"," 'Umesh',\n"," 'Umesh',\n"," 'Umesh',\n"," 'Umesh',\n"," 'Umesh',\n"," 'dhoni',\n"," 'dhoni',\n"," 'dhoni',\n"," 'dhoni',\n"," 'dhoni',\n"," 'dhoni',\n"," 'dhoni',\n"," 'dhoni',\n"," 'dhoni',\n"," 'shewag',\n"," 'shewag',\n"," 'shewag',\n"," 'shewag',\n"," 'shewag',\n"," 'shewag',\n"," 'shewag',\n"," 'shewag',\n"," 'shewag',\n"," 'jadeja',\n"," 'jadeja',\n"," 'jadeja',\n"," 'jadeja',\n"," 'jadeja',\n"," 'jadeja',\n"," 'jadeja',\n"," 'jadeja',\n"," 'jadeja',\n"," 'Yuvraj',\n"," 'Yuvraj',\n"," 'Yuvraj',\n"," 'Yuvraj',\n"," 'Yuvraj',\n"," 'Yuvraj',\n"," 'Yuvraj',\n"," 'Yuvraj',\n"," 'Yuvraj',\n"," 'Yuvraj']"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"tICXaU-xK4jk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"25e9350b-04c3-4383-c99e-270f382c0699","executionInfo":{"status":"ok","timestamp":1573976408088,"user_tz":-330,"elapsed":1218,"user":{"displayName":"AVARI FARZAD","photoUrl":"","userId":"06062050648998271274"}}},"source":["# dump the facial embeddings + names to disk\n","print(\"[INFO] serializing {} encodings ....\".format(total))\n","data = {\"paths\":ImgPaths, \"names\":names, \"imageIDs\":imageIDs, \"boxs\":boxs, \"embeddings\":embeddings}\n","file_path = \"/content/output/SimpleEmbeddings.pickle\"\n","with open(file_path , \"wb\") as f:\n","    pickle.dump(data,f)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[INFO] serializing 176 encodings ....\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HezAdkzVBbG4","colab":{}},"source":["from sklearn.preprocessing import LabelEncoder\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import LinearSVC\n","import pickle\n","import numpy as np\n","\n","# paths to embedding pickle file\n","embeddingPickle = \"./output/SimpleEmbeddings.pickle\"\n","\n","# path to recognizer pickle file\n","!touch output/SimpleRecognizer.pickle\n","recognizerPickle = \"./output/SimpleRecognizer.pickle\"\n","\n","# path to labels pickle file\n","!touch output/SimpleLabel.pickle\n","labelPickle = \"./output/SimpleLabel.pickle\"\n","\n","# loading embeddings pickle\n","data = pickle.loads(open(embeddingPickle, \"rb\").read())\n","\n","# encode the labels\n","label = LabelEncoder()\n","labels = label.fit_transform(data[\"names\"])\n","\n","# getting embeddings\n","Embeddings = np.array(data[\"embeddings\"])\n","\n","# train the model used to accept the 512-d embeddings of the face and \n","# then produce the actual face recognition\n","\n","recognizer = KNeighborsClassifier(n_neighbors= 2, metric='euclidean', weights=\"distance\")\n","#recognizer = SVC(probability=True)\n","recognizer.fit(Embeddings, labels)\n","\n","# write the actual face recognition model to disk\n","f = open(recognizerPickle, \"wb\")\n","f.write(pickle.dumps(recognizer))\n","f.close()\n","\n","# write the label encoder to disk\n","f = open(labelPickle,\"wb\")\n","f.write(pickle.dumps(label))\n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vH19Lo__E--a","colab":{}},"source":["# loading face detection model\n","detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n","\n","# load embedding model\n","embedder = embedding.InceptionResnetV1(pretrained=\"vggface2\").eval()\n","\n","# load the actual face recognition model along with the label encoder\n","recognizer = pickle.loads(open(recognizerPickle, \"rb\").read())\n","label = pickle.loads(open(labelPickle, \"rb\").read())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KBS5V1eBFiyr","outputId":"8860c379-20ad-4961-b950-94c355ca495d","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1573976892187,"user_tz":-330,"elapsed":2234,"user":{"displayName":"AVARI FARZAD","photoUrl":"","userId":"06062050648998271274"}}},"source":["imagePath = os.getcwd() + \"/test2.jpg\"\n","\n","predictedImg = os.getcwd()\n","\n","image = cv2.imread(imagePath)\n","(h,w) = image.shape[:2]\n","\n","blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n","\n","detector.setInput(blob)\n","detections = detector.forward()\n","\n","# loop over the detections\n","for i in range(0, detections.shape[2]):\n","    \n","    # extract the confidence (i.e., probalility) associated with the prediction\n","    confidence = detections[0, 0, i, 2]\n","    \n","    # fillter out weak detections\n","    if confidence > 0.2:\n","        \n","        # compute the (x ,y) - coordinates of the bounding box for the face\n","        \n","        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","        (startX, startY, endX, endY) = box.astype(\"int\")\n","        \n","        # extract the face ROI\n","        face = image[startY:endY , startX:endX]\n","        (fH ,fW) = face.shape[:2]\n","        \n","        # ensure the facce width and height are sufficently large\n","        if fW < 20 or fH < 20:\n","            print(\"[Error] - Face size in Image not sufficent to get Embeddings : \", imagePath)\n","            continue\n","        \n","\n","        try:\n","            faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,(160, 160), (0, 0, 0), swapRB=True, crop=False)\n","        except:\n","            print(\"[Error] - Face size in Image not sufficent to get Embeddings : \", imagePath)\n","            continue\n","        \n","        faceTensor = torch.tensor(faceBlob)\n","        faceEmbed = embedder(faceTensor)\n","        flattenEmbed = faceEmbed.squeeze(0).detach().numpy()\n","        \n","        array = np.array(flattenEmbed).reshape(1,-1)\n","        \n","        # perform classification to recognize the face\n","        \n","        preds = recognizer.predict_proba(array)[0]\n","        \n","        j = np.argmax(preds)\n","        \n","        proba = preds[j]\n","        name = label.classes_[j]\n","        \n","        #draw the bunding box of the face along with the associated probability\n","        \n","        text = \"{}: {:.2f}%\".format(name, proba * 100)\n","        y = startY - 10 if startY - 10 > 10 else startY + 10\n","        \n","        cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n","        \n","        cv2.putText(image, text, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.50, (255, 255, 255), 1)\n","        \n","# save image predicte folder\n","cv2.imwrite(\"{}/test_prediction.png\".format(predictedImg), image)"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"5jxn2sIsMVJY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}