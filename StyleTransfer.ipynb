{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "StyleTransfer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meet2147/Advanced-Computer-Vision/blob/master/StyleTransfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HEqsV8KMK29H",
        "colab": {}
      },
      "source": [
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import argparse\n",
        "import torch\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# define the VGG\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        \n",
        "        # load the vgg model's features\n",
        "        self.vgg = models.vgg19(pretrained=True).features\n",
        "    \n",
        "    def get_content_activations(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "            Extracts the features for the content loss from the block4_conv2 of VGG19\n",
        "            Args:\n",
        "                x: torch.Tensor - input image we want to extract the features of\n",
        "            Returns:\n",
        "                features: torch.Tensor - the activation maps of the block4_conv2 layer\n",
        "        \"\"\"\n",
        "        features = self.vgg[:23](x)\n",
        "        return features\n",
        "    \n",
        "    def get_style_activations(self, x):\n",
        "        \"\"\"\n",
        "            Extracts the features for the style loss from the block1_conv1, \n",
        "                block2_conv1, block3_conv1, block4_conv1, block5_conv1 of VGG19\n",
        "            Args:\n",
        "                x: torch.Tensor - input image we want to extract the features of\n",
        "            Returns:\n",
        "                features: list - the list of activation maps of the block1_conv1, \n",
        "                    block2_conv1, block3_conv1, block4_conv1, block5_conv1 layers\n",
        "        \"\"\"\n",
        "        features = [self.vgg[:4](x)] + [self.vgg[:7](x)] + [self.vgg[:12](x)] + [self.vgg[:21](x)] + [self.vgg[:30](x)] \n",
        "        return features\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.vgg(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GH6fakD0LBOj",
        "colab": {}
      },
      "source": [
        "def gram(tensor):\n",
        "    \"\"\"\n",
        "        Constructs the Gramian matrix out of the tensor\n",
        "    \"\"\"\n",
        "    return torch.mm(tensor, tensor.t())\n",
        "\n",
        "\n",
        "def gram_loss(noise_img_gram, style_img_gram, N, M):\n",
        "    \"\"\"\n",
        "        Gramian loss: the SSE between Gramian matrices of a layer\n",
        "            arXiv:1508.06576v2 - equation (4)\n",
        "    \"\"\"\n",
        "    return torch.sum(torch.pow(noise_img_gram - style_img_gram, 2)).div((np.power(N*M*2, 2, dtype=np.float64)))\n",
        "\n",
        "\n",
        "def total_variation_loss(image):\n",
        "    \"\"\"\n",
        "        Variation loss makes the images smoother, defined over spacial dimensions\n",
        "    \"\"\"\n",
        "    loss = torch.mean(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:])) + \\\n",
        "        torch.mean(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :]))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def content_loss(noise: torch.Tensor, image: torch.Tensor):\n",
        "    \"\"\"\n",
        "        Simple SSE loss over the generated image and the content image\n",
        "            arXiv:1508.06576v2 - equation (1)\n",
        "    \"\"\"\n",
        "    return 1/2. * torch.sum(torch.pow(noise - image, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xevv1Tp2Ln1U",
        "colab": {}
      },
      "source": [
        "def main(style_img_path: str,\n",
        "         content_img_path: str, \n",
        "         img_dim: int,\n",
        "         num_iter: int,\n",
        "         style_weight: int,\n",
        "         content_weight: int,\n",
        "         variation_weight: int,\n",
        "         print_every: int,\n",
        "         save_every: int):\n",
        "\n",
        "    assert style_img_path is not None\n",
        "    assert content_img_path is not None\n",
        "\n",
        "    # define the device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    # read the images\n",
        "    style_img = Image.open(style_img_path)\n",
        "    cont_img = Image.open(content_img_path)\n",
        "    \n",
        "    # define the transform\n",
        "    transform = transforms.Compose([transforms.Resize((img_dim, img_dim)),\n",
        "                                    transforms.ToTensor(), \n",
        "                                    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                         [0.229, 0.224, 0.225])])\n",
        "    \n",
        "    # get the tensor of the image\n",
        "    content_image = transform(cont_img).unsqueeze(0).to(device)\n",
        "    style_image = transform(style_img).unsqueeze(0).to(device)\n",
        "    \n",
        "    # init the network\n",
        "    vgg = VGG().to(device).eval()\n",
        "    \n",
        "    # replace the MaxPool with the AvgPool layers\n",
        "    for name, child in vgg.vgg.named_children():\n",
        "        if isinstance(child, nn.MaxPool2d):\n",
        "            vgg.vgg[int(name)] = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            \n",
        "    # lock the gradients\n",
        "    for param in vgg.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    # get the content activations of the content image and detach them from the graph\n",
        "    content_activations = vgg.get_content_activations(content_image).detach()\n",
        "    \n",
        "    # unroll the content activations\n",
        "    content_activations = content_activations.view(512, -1)\n",
        "    \n",
        "    # get the style activations of the style image\n",
        "    style_activations = vgg.get_style_activations(style_image)\n",
        "    \n",
        "    # for every layer in the style activations\n",
        "    for i in range(len(style_activations)):\n",
        "\n",
        "        # unroll the activations and detach them from the graph\n",
        "        style_activations[i] = style_activations[i].squeeze().view(style_activations[i].shape[1], -1).detach()\n",
        "\n",
        "    # calculate the gram matrices of the style image\n",
        "    style_grams = [gram(style_activations[i]) for i in range(len(style_activations))]\n",
        "    \n",
        "    # generate the Gaussian noise\n",
        "    noise = torch.randn(1, 3, img_dim, img_dim, device=device, requires_grad=True)\n",
        "    \n",
        "    # define the adam optimizer\n",
        "    # pass the feature map pixels to the optimizer as parameters\n",
        "    adam = optim.Adam(params=[noise], lr=0.01, betas=(0.9, 0.999))\n",
        "\n",
        "    # run the iteration\n",
        "    for iteration in range(num_iter):\n",
        "\n",
        "        # zero the gradient\n",
        "        adam.zero_grad()\n",
        "\n",
        "        # get the content activations of the Gaussian noise\n",
        "        noise_content_activations = vgg.get_content_activations(noise)\n",
        "\n",
        "        # unroll the feature maps of the noise\n",
        "        noise_content_activations = noise_content_activations.view(512, -1)\n",
        "\n",
        "        # calculate the content loss\n",
        "        content_loss_ = content_loss(noise_content_activations, content_activations)\n",
        "\n",
        "        # get the style activations of the noise image\n",
        "        noise_style_activations = vgg.get_style_activations(noise)\n",
        "\n",
        "        # for every layer\n",
        "        for i in range(len(noise_style_activations)):\n",
        "\n",
        "            # unroll the the noise style activations\n",
        "            noise_style_activations[i] = noise_style_activations[i].squeeze().view(noise_style_activations[i].shape[1], -1)\n",
        "\n",
        "        # calculate the noise gram matrices\n",
        "        noise_grams = [gram(noise_style_activations[i]) for i in range(len(noise_style_activations))]\n",
        "\n",
        "        # calculate the total weighted style loss\n",
        "        style_loss = 0\n",
        "        for i in range(len(style_activations)):\n",
        "            N, M = noise_style_activations[i].shape[0], noise_style_activations[i].shape[1]\n",
        "            style_loss += (gram_loss(noise_grams[i], style_grams[i], N, M) / 5.)\n",
        "\n",
        "        # put the style loss on device\n",
        "        style_loss = style_loss.to(device)\n",
        "            \n",
        "        # calculate the total variation loss\n",
        "        variation_loss = total_variation_loss(noise).to(device)\n",
        "\n",
        "        # weight the final losses and add them together\n",
        "        total_loss = content_weight * content_loss_ + style_weight * style_loss + variation_weight * variation_loss\n",
        "\n",
        "        if iteration % print_every == 0:\n",
        "            print(\"Iteration: {}, Content Loss: {:.3f}, Style Loss: {:.3f}, Var Loss: {:.3f}\".format(iteration, \n",
        "                                                                                                     content_weight * content_loss_.item(),\n",
        "                                                                                                     style_weight * style_loss.item(), \n",
        "                                                                                                     variation_weight * variation_loss.item()))\n",
        "\n",
        "        # create the folder for the generated images\n",
        "        if not os.path.exists('./generated/'):\n",
        "            os.mkdir('./generated/')\n",
        "        \n",
        "        # generate the image\n",
        "        if iteration % save_every == 0:\n",
        "            save_image(noise.cpu().detach(), filename='./generated/iter_{}.png'.format(iteration))\n",
        "\n",
        "        # backprop\n",
        "        total_loss.backward()\n",
        "        \n",
        "        # update parameters\n",
        "        adam.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XaRtt96IMkPk",
        "outputId": "ae996138-a6ca-4b2f-a02a-d3f932e66e7b",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-107ec612-4eef-40cb-9fc3-ece10ba68cb0\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-107ec612-4eef-40cb-9fc3-ece10ba68cb0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving lion2.jpeg to lion2.jpeg\n",
            "Saving style1.jpeg to style1.jpeg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S9uOVjojNlwI",
        "outputId": "df9118c9-61aa-4b16-93ad-8dbac5693ca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lion2.jpeg  sample_data  style1.jpeg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5PflNHxPPAW3",
        "outputId": "5475a18f-5f3f-4f8c-e928-e6bc0644ea51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import os\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "style_img = 'style1.jpeg'\n",
        "content_img = 'lion2.jpeg'\n",
        "\n",
        "main(style_img, content_img, 512, 12000, 10e6, 10e-4, 10e3, 500, 1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0, Content Loss: 473.797, Style Loss: 4673987.329, Var Loss: 22583.666\n",
            "Iteration: 500, Content Loss: 640.580, Style Loss: 31548.871, Var Loss: 17612.829\n",
            "Iteration: 1000, Content Loss: 658.608, Style Loss: 8488.238, Var Loss: 15576.034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DWazIvW21brs",
        "outputId": "63ce8fc0-812b-42fc-9fd2-27dcf2072c69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "!ls /content/generated/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter_0.png  iter_1000.png  iter_2000.png  iter_3000.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x9TkhXjr1nYr",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/generated/iter_3000.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTllZGYoIusT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}